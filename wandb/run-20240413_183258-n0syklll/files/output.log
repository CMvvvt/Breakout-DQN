1000
2000
3000
4000
5000
6000
7000
8000
9000
10000
11000
12000
13000
14000
15000
16000
17000
18000
19000
20000
21000
22000
23000
24000
25000
26000
27000
28000
29000
30000
31000
32000
33000
34000
35000
36000
37000
38000
39000
40000
41000
42000
43000
44000
45000
46000
47000
48000
49000
50000
51000
52000
53000
54000
55000
56000
57000
58000
59000
60000
61000
62000
63000
64000
65000
66000
67000
68000
69000
70000
71000
72000
73000
74000
75000
76000
77000
78000
79000
80000
81000
82000
83000
84000
85000
86000
87000
88000
89000
90000
91000
92000
93000
94000
95000
96000
97000
98000
99000
100000
Saving training model...









982it [00:19, 51.64it/s]










2088it [00:39, 45.14it/s]











3337it [01:01, 66.21it/s]






3986it [01:14, 59.73it/s]







4710it [01:27, 58.86it/s]




5191it [01:35, 58.76it/s]









5949it [01:54, 58.41it/s]



6249it [02:00, 56.18it/s]Traceback (most recent call last):
  File "/Users/ming/Courses/5180/project/dqn-breakout/main.py", line 133, in <module>
    main()
  File "/Users/ming/Courses/5180/project/dqn-breakout/main.py", line 97, in main
    loss = agent.learn()
  File "/Users/ming/Courses/5180/project/dqn-breakout/agent.py", line 97, in learn
    states, actions, rewards, next_states, dones = self.sample_batch()
  File "/Users/ming/Courses/5180/project/dqn-breakout/agent.py", line 57, in sample_batch
    next_states = torch.tensor(next_states, dtype=torch.float32).to(
KeyboardInterrupt
Traceback (most recent call last):
  File "/Users/ming/Courses/5180/project/dqn-breakout/main.py", line 133, in <module>
    main()
  File "/Users/ming/Courses/5180/project/dqn-breakout/main.py", line 97, in main
    loss = agent.learn()
  File "/Users/ming/Courses/5180/project/dqn-breakout/agent.py", line 97, in learn
    states, actions, rewards, next_states, dones = self.sample_batch()
  File "/Users/ming/Courses/5180/project/dqn-breakout/agent.py", line 57, in sample_batch
    next_states = torch.tensor(next_states, dtype=torch.float32).to(
KeyboardInterrupt