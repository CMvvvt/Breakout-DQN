1000
2000
3000
4000
5000
6000
7000
8000
9000
10000
11000
12000
13000
14000
15000
16000
17000
18000
19000
20000
21000
22000
23000
24000
25000
26000
27000
28000
29000
30000
31000
32000
33000
34000
35000
36000
37000
38000
39000
40000
41000
42000
43000
44000
45000
46000
47000
48000
49000
50000
51000
52000
53000
54000
55000
56000
57000
58000
59000
60000
61000
62000
63000
64000
65000
66000
67000
68000
69000
70000
71000
72000
73000
74000
75000
76000
77000
78000
79000
80000
81000
82000
83000
84000
85000
86000
87000
88000
89000
90000
91000
92000
93000
94000
95000
96000
97000
98000
99000
100000



501it [00:08, 71.41it/s]/Users/ming/Courses/5180/project/dqn-breakout/agent.py:68: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:278.)
  state = torch.tensor([observation], dtype=torch.float32).to(
567it [00:09, 34.67it/s]
reward of episode 0: 1.0




1104it [00:17, 72.57it/s]
reward of episode 1: 0.0




1618it [00:25, 60.41it/s]
reward of episode 2: 0.0








2682it [00:41, 70.52it/s]
reward of episode 3: 3.0



3048it [00:47, 69.98it/s]
reward of episode 4: 0.0





3755it [00:57, 71.03it/s]
reward of episode 5: 1.0





4317it [01:07, 55.60it/s]
reward of episode 6: 0.0





4933it [01:17, 66.96it/s]
reward of episode 7: 1.0






5739it [01:30, 66.06it/s]
reward of episode 8: 2.0







6512it [01:44, 60.79it/s]
reward of episode 9: 2.0





7167it [01:53, 68.44it/s]
reward of episode 10: 2.0




7692it [02:01, 56.16it/s]
reward of episode 11: 0.0






8460it [02:13, 61.83it/s]
reward of episode 12: 2.0




8968it [02:22, 54.89it/s]
reward of episode 13: 0.0







9838it [02:36, 64.48it/s]
reward of episode 14: 2.0



10225it [02:42, 58.89it/s]
reward of episode 15: 0.0




10729it [02:50, 64.47it/s]
reward of episode 16: 0.0











12111it [03:11, 68.66it/s]
reward of episode 17: 5.0






12879it [03:24, 69.53it/s]
reward of episode 18: 2.0






13651it [03:36, 63.39it/s]
reward of episode 19: 2.0




14167it [03:44, 64.78it/s]
reward of episode 20: 0.0







15018it [03:58, 67.13it/s]
reward of episode 21: 2.0




15495it [04:06, 48.50it/s]
reward of episode 22: 0.0




16043it [04:14, 70.72it/s]
reward of episode 23: 0.0








17020it [04:30, 70.35it/s]
reward of episode 24: 3.0




17530it [04:38, 68.48it/s]
reward of episode 25: 0.0




18054it [04:46, 49.00it/s]
reward of episode 26: 0.0







19000it [05:00, 68.17it/s]
reward of episode 27: 3.0





19655it [05:10, 67.74it/s]
reward of episode 28: 1.0




20182it [05:18, 71.50it/s]
reward of episode 29: 0.0




20692it [05:26, 62.75it/s]
reward of episode 30: 1.0







21586it [05:40, 69.88it/s]
reward of episode 31: 2.0




22091it [05:48, 59.60it/s]
reward of episode 32: 0.0








23044it [06:04, 64.19it/s]
reward of episode 33: 3.0




23497it [06:12, 60.16it/s]
reward of episode 34: 0.0





24123it [06:22, 62.80it/s]
reward of episode 35: 1.0




24650it [06:30, 56.90it/s]
reward of episode 36: 0.0




25130it [06:38, 65.41it/s]
reward of episode 37: 0.0




25607it [06:46, 56.45it/s]
reward of episode 38: 0.0









26723it [07:05, 60.01it/s]
reward of episode 39: 3.0
26798it [07:06, 51.13it/s]Traceback (most recent call last):
  File "/Users/ming/Courses/5180/project/dqn-breakout/main.py", line 104, in <module>
    main()
  File "/Users/ming/Courses/5180/project/dqn-breakout/main.py", line 76, in main
    loss = agent.learn()
  File "/Users/ming/Courses/5180/project/dqn-breakout/agent.py", line 104, in learn
  File "/Users/ming/.pyenv/versions/3.10.13/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper
    out = func(*args, **kwargs)
  File "/Users/ming/.pyenv/versions/3.10.13/lib/python3.10/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/Users/ming/.pyenv/versions/3.10.13/lib/python3.10/site-packages/torch/optim/adam.py", line 166, in step
    adam(
  File "/Users/ming/.pyenv/versions/3.10.13/lib/python3.10/site-packages/torch/optim/adam.py", line 316, in adam
    func(params,
  File "/Users/ming/.pyenv/versions/3.10.13/lib/python3.10/site-packages/torch/optim/adam.py", line 439, in _single_tensor_adam
    denom = (exp_avg_sq.sqrt() / bias_correction2_sqrt).add_(eps)
KeyboardInterrupt
Traceback (most recent call last):
  File "/Users/ming/Courses/5180/project/dqn-breakout/main.py", line 104, in <module>
    main()
  File "/Users/ming/Courses/5180/project/dqn-breakout/main.py", line 76, in main
    loss = agent.learn()
  File "/Users/ming/Courses/5180/project/dqn-breakout/agent.py", line 104, in learn
  File "/Users/ming/.pyenv/versions/3.10.13/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper
    out = func(*args, **kwargs)
  File "/Users/ming/.pyenv/versions/3.10.13/lib/python3.10/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/Users/ming/.pyenv/versions/3.10.13/lib/python3.10/site-packages/torch/optim/adam.py", line 166, in step
    adam(
  File "/Users/ming/.pyenv/versions/3.10.13/lib/python3.10/site-packages/torch/optim/adam.py", line 316, in adam
    func(params,
  File "/Users/ming/.pyenv/versions/3.10.13/lib/python3.10/site-packages/torch/optim/adam.py", line 439, in _single_tensor_adam
    denom = (exp_avg_sq.sqrt() / bias_correction2_sqrt).add_(eps)
KeyboardInterrupt